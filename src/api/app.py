import os
import httpx # requests ëŒ€ì‹  ì‚¬ìš©í•˜ëŠ” ë¹„ë™ê¸° ë¼ì´ë¸ŒëŸ¬ë¦¬ (pip install httpx)
import asyncio
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware # CORS ë¯¸ë“¤ì›¨ì–´
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# --- LangChain ê´€ë ¨ ---
from langchain_community.vectorstores import FAISS
# from langchain_community.embeddings import HuggingFaceEmbeddings # ë¡œì»¬ ëª¨ë¸ ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ
from langchain_openai import ChatOpenAI, OpenAIEmbeddings # OpenAI ì‚¬ìš© ì‹œ
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser

# --- .env ë¡œë“œ ---
load_dotenv()

# --- 1. ì„¤ì • ---
# â˜… ì¤‘ìš”: DB ë§Œë“¤ ë•Œ ì“´ ëª¨ë¸ê³¼ ë˜‘ê°™ì€ ê±¸ ì¨ì•¼ í•©ë‹ˆë‹¤!
# DB_FAISS_PATH = "faiss_index"         # ë¡œì»¬ ëª¨ë¸(KURE)ë¡œ ë§Œë“  DB ê²½ë¡œ
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DB_FAISS_PATH = os.path.join(BASE_DIR, "faiss_index_openai")  # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
USE_OPENAI_EMBEDDING = True           # Trueë©´ OpenAI, Falseë©´ KURE(ë¡œì»¬)

# ì „ì—­ ë³€ìˆ˜ (DB, Embeddings, LLM)
resources = {}

# --- 2. Lifespan (ì„œë²„ ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    # [ì‹œì‘ ì‹œ ì‹¤í–‰]
    print("ğŸš€ FastAPI ì„œë²„ ì‹œì‘ ì¤‘...")
    
    # 1. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    try:
        if USE_OPENAI_EMBEDDING:
            print("Settings: OpenAI Embeddings (text-embedding-3-small)")
            resources['embeddings'] = OpenAIEmbeddings(model="text-embedding-3-small")
        else:
            print("Settings: HuggingFace Embeddings (nlpai-lab/KURE-v1)")
            # resources['embeddings'] = HuggingFaceEmbeddings(
            #     model_name="nlpai-lab/KURE-v1",
            #     model_kwargs={'device': 'cpu'}
            # )
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

    # 2. Vector DB ë¡œë“œ
    if os.path.exists(DB_FAISS_PATH):
        try:
            print(f"ğŸ“‚ Vector DB ë¡œë“œ ì¤‘: {DB_FAISS_PATH}")
            resources['db'] = FAISS.load_local(
                DB_FAISS_PATH, 
                resources['embeddings'],
                allow_dangerous_deserialization=True
            )
            print("âœ… Vector DB ë¡œë“œ ì™„ë£Œ.")
        except Exception as e:
            print(f"âŒ Vector DB ë¡œë“œ ì—ëŸ¬: {e}")
            resources['db'] = None
    else:
        print(f"âš ï¸ ê²½ê³ : '{DB_FAISS_PATH}' ê²½ë¡œì— DBê°€ ì—†ìŠµë‹ˆë‹¤.")
        resources['db'] = None

    # 3. LLM ì´ˆê¸°í™”
    resources['llm'] = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    yield # ì—¬ê¸°ì„œë¶€í„° ì„œë²„ ê°€ë™

    # [ì¢…ë£Œ ì‹œ ì‹¤í–‰]
    print("ğŸ‘‹ ì„œë²„ ì¢…ë£Œ. ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.")
    resources.clear()

# --- 3. FastAPI ì•± ìƒì„± ---
app = FastAPI(
    title="FactCheck RAG API",
    description="ë‰´ìŠ¤ ê¸°ì‚¬ íŒ©íŠ¸ì²´í¬ ë° ìœ ì‚¬ë„ ê²€ìƒ‰ API",
    lifespan=lifespan
)

# CORS ì„¤ì • (í”„ë¡ íŠ¸ì—”ë“œ ì—°ë™ í•„ìˆ˜)
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "*", # ê°œë°œ í¸ì˜ë¥¼ ìœ„í•´ ìœ ì§€í•˜ë˜, ì•„ë˜ì— ëª…ì‹œì  ì¶œì²˜ ì¶”ê°€
        "https://n.news.naver.com",
        "https://news.naver.com",
        "https://m.news.naver.com",
    ], 
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 4. ë°ì´í„° ëª¨ë¸ ---
# [ì‚­ì œ/ì£¼ì„] GET ë°©ì‹ì—ì„œëŠ” Request Bodyìš© ëª¨ë¸ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
# class FactCheckRequest(BaseModel):
#     url: str 

class FactCheckResponse(BaseModel):
    original_claims: List[Dict[str, str]]
    related_factchecks: List[Dict[str, Any]]

class SearchResult(BaseModel):
    content: str
    metadata: Dict[str, Any]
    score: float

class SearchResponse(BaseModel):
    query: str
    results: List[SearchResult]

class EmbedResponse(BaseModel):
    text: str
    vector: List[float]

# --- 5. ë¹„ë™ê¸° í—¬í¼ í•¨ìˆ˜ ---

async def crawl_naver_news_async(url: str) -> str:
    """
    httpxë¥¼ ì‚¬ìš©í•œ ì§„ì •í•œ ë¹„ë™ê¸° í¬ë¡¤ë§
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    try:
        async with httpx.AsyncClient() as client:
            # follow_redirects=True: ë‹¨ì¶• URL ë“± ë¦¬ë‹¤ì´ë ‰íŠ¸ ìë™ ì²˜ë¦¬
            response = await client.get(url, headers=headers, follow_redirects=True, timeout=10.0)
            response.raise_for_status()
            html = response.text
            
            soup = BeautifulSoup(html, 'html.parser')
            # ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§
            content = soup.select_one('#dic_area')
            if not content:
                content = soup.select_one('#articeBody')
            
            if content:
                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                for tag in content(['script', 'style', 'iframe', 'button']):
                    tag.decompose()
                return content.get_text(strip=True)
            return ""
    except Exception as e:
        print(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        return ""

async def extract_claims_async(text: str):
    """
    ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ ì£¼ì¥ ì¶”ì¶œ (LLM)
    """
    if not text or len(text) < 50:
        return []

    system_prompt = """ë‹¹ì‹ ì€ íŒ©íŠ¸ì²´í¬ë¥¼ ìœ„í•œ ë‰´ìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ 'ê²€ì¦ ê°€ëŠ¥í•œ í•µì‹¬ ì£¼ì¥(Claim)'ì„ 3ê°œ ì¶”ì¶œí•˜ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹ - JSON Only]
[
  {{
    "claim": "ì£¼ì¥ ë‚´ìš© (í•œ ë¬¸ì¥)",
    "type": "Fact" ë˜ëŠ” "Opinion",
    "query": "ê²€ìƒ‰ìš© ì¿¼ë¦¬ (í•µì‹¬ í‚¤ì›Œë“œ ìœ„ì£¼)"
  }}
]
"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{text}")
    ])
    
    # ë¦¬ì†ŒìŠ¤ì—ì„œ LLM ê°€ì ¸ì˜¤ê¸°
    llm = resources.get('llm')
    if not llm: return []

    chain = prompt | llm | JsonOutputParser()
    
    try:
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ë³´ëƒ„ (í† í° ì ˆì•½)
        return await chain.ainvoke({"text": text[:3500]})
    except Exception as e:
        print(f"í´ë ˆì„ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []

async def verify_claim_with_llm(claim: str, related_docs: list):
    """
    ì£¼ì¥ê³¼ ê²€ìƒ‰ëœ íŒ©íŠ¸ì²´í¬ ê¸°ì‚¬ë¥¼ ë¹„êµí•˜ì—¬ ì§„ìœ„ ì—¬ë¶€ë¥¼ íŒë‹¨
    """
    if not related_docs:
        return {"judgment": "íŒë‹¨ ë¶ˆê°€", "reason": "ê´€ë ¨ëœ íŒ©íŠ¸ì²´í¬ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.", "reference_index": []}

    # ê²€ìƒ‰ëœ ê¸°ì‚¬ ë‚´ìš© í•©ì¹˜ê¸°
    context = ""
    for i, doc in enumerate(related_docs):
        context += f"[ê¸°ì‚¬ {i+1}] (ì¶œì²˜: {doc['metadata'].get('press')})\n{doc['content']}\n\n"

    system_prompt = """ë‹¹ì‹ ì€ íŒ©íŠ¸ì²´í¬ ê²€ì¦ AIì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ 'ì£¼ì¥(Claim)'ê³¼ ì´ë¥¼ ê²€ì¦í•  ìˆ˜ ìˆëŠ” 'íŒ©íŠ¸ì²´í¬ ê¸°ì‚¬ë“¤(Context)'ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ê¸°ì‚¬ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ì¥ì´ 'ì‚¬ì‹¤', 'ê±°ì§“', 'íŒë‹¨ ë¶ˆê°€' ì¤‘ ë¬´ì—‡ì¸ì§€ íŒë³„í•˜ê³ , 
ë§Œì•½ 'ê±°ì§“'ì´ë¼ë©´ ê¸°ì‚¬ì˜ ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ì™œ í‹€ë ¸ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ë°˜ë°•í•˜ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹ - JSON]
{{
    "judgment": "ì‚¬ì‹¤" | "ê±°ì§“" | "íŒë‹¨ ë¶ˆê°€",
    "reason": "íŒë‹¨ ì´ìœ  ë° ë°˜ë°• ë‚´ìš© (3ë¬¸ì¥ ì´ë‚´)",
    "reference_index": [ì°¸ê³ í•œ ê¸°ì‚¬ ë²ˆí˜¸ (ì˜ˆ: 1, 2)]
}}
"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "ì£¼ì¥: {claim}\n\níŒ©íŠ¸ì²´í¬ ê¸°ì‚¬ë“¤:\n{context}")
    ])
    
    llm = resources.get('llm')
    chain = prompt | llm | JsonOutputParser()
    
    try:
        return await chain.ainvoke({"claim": claim, "context": context})
    except Exception as e:
        print(f"ê²€ì¦ ì˜¤ë¥˜: {e}")
        return {"judgment": "ì˜¤ë¥˜", "reason": "ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "reference_index": []}

async def process_single_claim(claim: Dict[str, str], db: Any) -> Optional[Dict[str, Any]]:
    """
    ê°œë³„ ì£¼ì¥ì— ëŒ€í•œ ê²€ìƒ‰ ë° ê²€ì¦ì„ ìˆ˜í–‰í•˜ëŠ” ë¹„ë™ê¸° í•¨ìˆ˜
    """
    query = claim.get('query')
    if not query: 
        return None

    # 1. DB ê²€ìƒ‰ (ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ ê³ ë ¤ ê°€ëŠ¥í•˜ë‚˜, FAISSê°€ ë¹ ë¥´ë©´ ê·¸ëƒ¥ ì‹¤í–‰)
    # LangChain FAISS wrapperëŠ” ë™ê¸° í•¨ìˆ˜ì„.
    docs_with_scores = db.similarity_search_with_score(query, k=2)
    
    search_hits = []
    for doc, score in docs_with_scores:
        # ê±°ë¦¬(Distance) ê¸°ë°˜ í•„í„°ë§
        if score > 1.2: 
            continue

        search_hits.append({
            "content": doc.page_content,
            "metadata": doc.metadata,
            "score": float(score)
        })
    
    # 2. LLM ê²€ì¦ (ë¹„ë™ê¸°)
    verification = await verify_claim_with_llm(claim.get('claim'), search_hits)
    
    return {
        "claim": claim.get('claim'),
        "query": query,
        "related_facts": search_hits,
        "verification": verification
    }

# --- 6. ì—”ë“œí¬ì¸íŠ¸ ---

@app.get("/check-facts", response_model=FactCheckResponse, summary="URL ê¸°ë°˜ íŒ©íŠ¸ì²´í¬ (GET)")
async def check_facts_by_url(url: str):
    """
    [GET] /check-facts?url=https://n.news.naver.com/... í˜•íƒœë¡œ ìš”ì²­
    """
    db = resources.get('db')
    if not db:
        raise HTTPException(status_code=503, detail="Vector DBê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # 1. í¬ë¡¤ë§
    article_text = await crawl_naver_news_async(url)
    if not article_text:
        raise HTTPException(status_code=400, detail="ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 2. í´ë ˆì„ ì¶”ì¶œ
    claims = await extract_claims_async(article_text)
    if not claims:
        # ì£¼ì¥ì´ ì•ˆ ë½‘í˜”ì„ ê²½ìš° ë¹ˆ ê²°ê³¼ ë°˜í™˜ ëŒ€ì‹  ì—ëŸ¬ ì²˜ë¦¬ ì„ íƒ ê°€ëŠ¥
        return FactCheckResponse(original_claims=[], related_factchecks=[])

    # 3. DB ê²€ìƒ‰ ë° ê²€ì¦ (ë³‘ë ¬ ì²˜ë¦¬)
    # ê° ì£¼ì¥ì— ëŒ€í•´ process_single_claimì„ ë™ì‹œì— ì‹¤í–‰
    tasks = [process_single_claim(claim, db) for claim in claims]
    results = await asyncio.gather(*tasks)
    
    # None ê²°ê³¼ í•„í„°ë§
    related_results = [res for res in results if res is not None]

    return FactCheckResponse(
        original_claims=claims,
        related_factchecks=related_results
    )

@app.get("/search", response_model=SearchResponse, summary="ë‹¨ìˆœ ê²€ìƒ‰")
def search_latent_space(q: str, k: int = 3):
    db = resources.get('db')
    if not db:
        raise HTTPException(status_code=503, detail="Vector DB Not Ready")
    
    docs_with_scores = db.similarity_search_with_score(q, k=k)
    
    results_list = []
    for doc, score in docs_with_scores:
        results_list.append(SearchResult(
            content=doc.page_content,
            metadata=doc.metadata,
            score=score
        ))
    
    return SearchResponse(query=q, results=results_list)

@app.get("/", summary="Health Check")
def read_root():
    return {"status": "OK", "model": "OpenAI" if USE_OPENAI_EMBEDDING else "Local"}